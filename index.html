<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <title>Minglin Chen</title>
  
    <meta name="author" content="Minglin Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px"><td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center"><name>Minglin Chen</name></p>
                <p>I am a Ph.D. Student at <a href="https://sysu-sail.github.io/">SAIL</a> Lab of <a href="https://www.sysu.edu.cn/">Sun Yat-sen University (SYSU)</a>, and supervised by <a href="https://www.yulanguo.cn/">Prof. Yulan Guo</a>. Prior to that, I obtained M.Eng. degree from <a href="https://www.ucas.edu.cn/">University of Chinese Academy of Sciences (UCAS)</a>, and B.Eng. degree from <a href="https://www.scnu.edu.cn/">South China Normal University (SCNU)</a>. </p>
                <p>My research interests are in 3D reconstruction and 3D generation. In particular, I am working on neural radiance fields (NeRF) and their combination with diffusion models (e.g., Stable Diffusion). </p>
                <p>Outside of research, I enjoy playing badminton, basketball, and swimming. </p>
                <p>ðŸ”¥ <strong style="color: red;">I plan to be available in the job market by June 2026. If you're interested in my work, feel free to reach out.</strong></p>
                <p style="text-align:center">
                    <a href="mailto:chenmlin8@mail2.sysu.edu.cn">Email</a> &nbsp|&nbsp
                    <a href="https://scholar.google.com/citations?user=G4oCNmUAAAAJ">Google Scholar</a> &nbsp|&nbsp
                    <a href="https://github.com/Minglin-Chen">Github</a> &nbsp|&nbsp
                    <a href="CV/MinglinChen_CV_CN.pdf">CV</a>
                </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/mlchen.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mlchen_circle.jpg" class="hoverZoomLink"></a>
            </td>
        </tr>
        </tbody></table>
    </td></tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr><td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Research</h2>
    </td></tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/graph2scene.jpg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Graph2Scene: Versatile 3D Indoor Scene Generation with Interaction-aware Scene Graph</strong>
            <br>
            <strong>Minglin Chen</strong>,
            <a href="#">Rongkun Yang</a>,
            <a href="#">Qibin Hu</a>,
            <a href="https://scholar.google.com/citations?user=0mRilxMAAAAJ&hl=zh-CN&oi=ao">Kaiwen Xue</a>,
            <a href="https://sites.google.com/view/shunbozhou">Shunbo Zhou</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>
            <br>
            <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2025
            <br>
            <p></p>
            <p>
            Graph2Scene learns to generate 3D indoor scenes with interaction-aware scene graphs.
            </p>
        </td>
    </tr>
    

    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/dropoutgs.svg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering</strong>
            <br>
            <a href="https://xuyx55.github.io/xuyx/">Yexing Xu*</a>,
            <a href="https://longguangwang.github.io/">Longguang Wang*</a>,
            <strong>Minglin Chen</strong>,
            <a href="https://scholar.google.com/citations?user=cvS1yuMAAAAJ&hl=zh-CN">Sheng Ao</a>,
            <a href="https://www.fst.um.edu.mo/personal/llili/">Li Li</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>
            <br>
            <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2025
            <br>
            <a href="https://xuyx55.github.io/DropoutGS/">Project page</a>
            |
            <a href="https://arxiv.org/abs/2504.09491">arXiv</a>
            |
            <a href="https://github.com/xuyx55/DropoutGS">Code</a>
            <p></p>
            <p>
            DropoutGS explores the use of dropping out Gaussians for better sparse-view rendering.
            </p>
        </td>
    </tr>


    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/layout2scene.jpg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Layout2Scene: 3D Semantic Layout Guided Scene Generation via Geometry and Appearance Diffusion Priors</strong>
            <br>
            <strong>Minglin Chen</strong>,
            <a href="https://longguangwang.github.io/">Longguang Wang</a>,
            <a href="https://scholar.google.com/citations?user=cvS1yuMAAAAJ&hl=zh-CN">Sheng Ao</a>,
            <a href="https://github.com/crocodilegogogo">Ye Zhang</a>,
            <a href="https://kevinkaixu.net/">Kai Xu</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>
            <br>
            <em>arXiv</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2501.02519">arXiv</a>
            <p></p>
            <p>
            Layout2Scene generates 3D realistic indoor scenes from 3D layouts.
            </p>
        </td>
    </tr>
    

    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/wssicnet.jpg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>WSSIC-Net: Weakly-Supervised Semantic Instance Completion of 3D Point Cloud Scenes</strong>
            <br>
            <a href="https://ieeexplore.ieee.org/author/37086343299">Zhiheng Fu</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>,
            <strong>Minglin Chen</strong>,
            <a href="https://qingyonghu.github.io/">Qingyong Hu</a>,
            <a href="https://researchportal.murdoch.edu.au/esploro/profile/hamid_laga/overview">Hamid Laga</a>,
            <a href="https://research-repository.uwa.edu.au/en/persons/farid-boussaid">Farid Boussaid</a>,
            <a href="https://research-repository.uwa.edu.au/en/persons/mohammed-bennamoun">Mohammed Bennamoun</a>
            <br>
            <em>IEEE Transactions on Image Processing (IEEE TIP)</em>, 2025
            <br>
            <a href="https://ieeexplore.ieee.org/abstract/document/10944293">Paper</a>
            <p></p>
            <p>
            WSSIC-Net is a weakly-supervised semantic instance completion model for 3D point cloud scenes.
            </p>
        </td>
    </tr>

    
    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/tangram.jpg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Tangram-Splatting: Optimizing 3D Gaussian Splatting Through Tangram-inspired Shape Priors</strong>
            <br>
            <a href="https://scholar.google.com/citations?user=NxgoWMQAAAAJ&hl=zh-CN&oi=sra">Yi Wang</a>,
            <a href="#">Ningze Zhong</a>,
            <strong>Minglin Chen</strong>,
            <a href="https://longguangwang.github.io/">Longguang Wang</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>
            <br>
            <em>ACM International Conference on Multimedia (ACM'MM)</em>, 2024
            <br>
            <a href="https://dl.acm.org/doi/10.1145/3664647.3680688">Paper</a>
            <p></p>
            <p>
            Tangram-Splatting explore different shape Gaussian for efficient novel view synthesis.
            </p>
        </td>
    </tr>

    
    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/meme.jpg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Distractor-free Novel View Synthesis via Exploiting Memorization Effect in Optimization</strong>
            <br>
            <a href="https://github.com/Yukun66?tab=repositories">Yukun Wang</a>,
            <a href="https://scholar.google.com/citations?user=_kzDdx8AAAAJ&hl=zh-CN&oi=ao">Kunhong Li</a>,
            <strong>Minglin Chen</strong>,
            <a href="https://longguangwang.github.io/">Longguang Wang</a>,
            <a href="https://sites.google.com/view/shunbozhou">Shunbo Zhou</a>,
            <a href="https://scholar.google.com/citations?user=0mRilxMAAAAJ&hl=zh-CN&oi=ao">Kaiwen Xue</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>
            <br>
            <em>The European Conference on Computer Vision (ECCV)</em>, 2024
            <br>
            <a href="https://eccv.ecva.net/virtual/2024/poster/906">Paper</a>
            |
            <a href="https://github.com/Yukun66/MemE">Code</a>
            <p></p>
            <p>
            The work explores the memorization effect in optimization for distractor-free novel view synthesis.
            </p>
        </td>
    </tr>


    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='Sketch2NeRF/static/sketch2nerf.jpg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</strong>
            <br>
            <strong>Minglin Chen</strong>,
            <a href="https://longguangwang.github.io/">Longguang Wang</a>,
            <a href="https://weihao-yuan.com/">Weihao Yuan</a>,
            <a href="https://github.com/Yukun66?tab=repositories">Yukun Wang</a>,
            <a href="https://scholar.google.com/citations?user=EnDHC38AAAAJ&hl=zh-CN&oi=sra">Zhe Sheng</a>,
            <a href="https://hyshkust.github.io/">Yisheng He</a>,
            <a href="http://www.cad.zju.edu.cn/home/zldong/">Zilong Dong</a>,
            <a href="https://research.cs.washington.edu/istc/lfb/">Liefeng Bo</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>
            <br>
            <em>arXiv</em>, 2024
            <br>
            <!-- <a href="./Sketch2NeRF/index.html">Project Page</a>
            | -->
            <a href="https://arxiv.org/abs/2401.14257v1">arXiv</a>
            <p></p>
            <p>
            Sketch2NeRF is a sketch-guided text-to-3D generative model that produces high-fidelity 3D objects resembling multi-view sketches.
            </p>
        </td>
    </tr>


    <tr class="research-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='SphericalRF/img/sphericalrf.jpg' width=100%>
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Learning Spherical Radiance Field for Efficient 360Â° Unbounded Novel View Synthesis</strong>
            <br>
            <strong>Minglin Chen</strong>,
            <a href="https://longguangwang.github.io/">Longguang Wang</a>,
            <a href="https://dblp.org/pid/28/9238.html">Yinjie Lei</a>,
            <a href="http://www.cad.zju.edu.cn/home/zldong/">Zilong Dong</a>,
            <a href="https://www.yulanguo.cn/">Yulan Guo</a>
            <br>
            <em>IEEE Transactions on Image Processing (IEEE TIP)</em>, 2024
            <br>
            <a href="./SphericalRF/index.html">Project page</a>
            |
            <a href="https://github.com/Minglin-Chen/SphericalRF">Code</a>
            <p></p>
            <p>
            We present the Spherical Radiance Field to achieve efficient novel view synthesis on 360Â° unbounded scene.  
            </p>
        </td>
    </tr>


    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr><td style="padding:20px;width:100%;vertical-align:middle">
        <h2>Experience</h2>
    </td></tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

    <tr class="experience-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/huawei_logo.webp' width=100% alt="Huawei Logo">
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Research Intern</strong>
            <br>
            <em>Huawei 2012 Laboratories</em>
            <br>
            <em>January 2021 - July 2021</em>
            <p></p>
            <p>
            Conducted research on human reconstruction from single image, focusing on implicit representation techniques.
            </p>
        </td>
    </tr>

    <tr class="experience-block">
        <td class="image-cell" style="padding:5px;width:20%;vertical-align:middle">
            <div class="one">
                <img src='asset/static/alibaba_logo.png' width=100% alt="Alibaba Logo">
            </div>
        </td>

        <td class="content-cell" style="padding:5px;width:80%;vertical-align:middle">
            <strong>Research Intern (Remote)</strong>
            <br>
            <em>Alibaba DAMO Academy</em>
            <br>
            <em>May 2022 - November 2023</em>
            <p></p>
            <p>
            Worked on panoramic neural radiance fields, and text-to-3D generation.
            </p>
        </td>
    </tr>

    </tbody></table>
</body>
  
</html>